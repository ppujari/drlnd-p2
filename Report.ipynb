{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ContinousControl Project` Report\n",
    "by: Pradeep Pujari (ppujari@gmail.com)\n",
    "\n",
    "Completion date: 18 Aug 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Environment\n",
    "\n",
    "Unity provides a variety of machine learning environments as part of their [`ml-agents`][mlagents] project. Particularly, the chosen project is referred to as `Reacher` &ndash; this is a Continous Control task, which is notably difficult for machine learning agents, especially reinforcement learners. This most challenging aspect of this task is that it has a continuous action space – which makes the actions highly dimensional, and discretizing it can further complicate things.\n",
    "\n",
    "The goal is to implement an algorithm that's been successful in Continuous Control environments and also benefits from \"shared learning\" between agents, rather than learning on an individual scale (as this can take extremely lengthy time periods).\n",
    "\n",
    "\n",
    "\n",
    "[mlagents]: https://github.com/Unity-Technologies/ml-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Algorithm\n",
    "\n",
    "I chose to use the DDPG algorithm (Deep Deterministic Policy Gradients), as described in [_Continuous Control with Deep Reinforcement Learning_][ddpg-paper] (Lillicrap et al). The foundation of this code-base is from the [Udacity DRL `ddpg-bipedal` notebook][ddpg-repo].\n",
    "\n",
    "[ddpg-paper]: https://arxiv.org/pdf/1509.02971.pdf\n",
    "[ddpg-repo]: https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-bipedal/DDPG.ipynb\n",
    "\n",
    "Completed both single agent and multi agent version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "#single agent\n",
    "#env = UnityEnvironment(file_name='../reacher_1_agent/Reacher', worker_id=0, seed=2)\n",
    "## 20 agents\n",
    "env = UnityEnvironment(file_name='../Reacher', worker_id=0, seed=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space, per agent, consists of 33 variables – these all correspond to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints – every action vector value is between `\\[-1, 1\\]`.\n",
    "\n",
    "However, I modified the `ddpg-bipedal` code by adding connective code to interact with `Reacher`, scaling up to 20-agents, implementing checkpointing for each `Actor` and `Critic`, adding environmental completion logic, and plotting average score (of 20-agents) over 400 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodic breakdown\n",
    "```python    \n",
    "## ... initial setup\n",
    "env_solv = False\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "states = env_info.vector_observations\n",
    "\n",
    "agents = [Agent(**agent_kwargs) for _ in range(n_agents)]\n",
    "action = [agent.act(states[idx]) for idx, agent in enumerate(agents)]\n",
    "for epis in range(1, n_epis + 1):\n",
    "\n",
    "    states = env_info.vector_observations\n",
    "\n",
    "    ## reset the agent\n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "\n",
    "    ## ... time-step loop\n",
    "\n",
    "    ## ... progress output and checkpointing\n",
    "```\n",
    "1. This resets the Unity environment and obtains about the current state, current reward, and the completion state of current episode for each of the 20 arms in the `Reacher` environment.\n",
    "1. Records the current state information (as provided in `env_info`).\n",
    "1. Initializes the score for the current episode to zero.\n",
    "1. And, finally, executes a set of time steps making up a single episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-step breakdown\n",
    "This can run up to 500 time-steps &ndash; an arbitrary \"time\" limit.\n",
    "```python\n",
    "## ... time-step loop\n",
    "for t in range(max_t):\n",
    "    actions = [agent.act(states[idx]) for idx, agent in enumerate(agents)]\n",
    "\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "\n",
    "    step_tuple = zip(agents, states, actions, rewards, next_states, dones)\n",
    "    for agent, s, a, r, s_, d in step_tuple:\n",
    "        agent.step(s, a, r, s_, d)\n",
    "\n",
    "    states  = next_states\n",
    "    scores += rewards\n",
    "\n",
    "    if np.any(dones):\n",
    "        break\n",
    "```\n",
    "1. For each agent, obtain the next action (based on current state and the current value of $\\epsilon$.\n",
    "1. Move to the next time-step, taking the actions just obtained.\n",
    "1. Gather `next_states`, `rewards`, and `done` values.\n",
    "1. Take a learning step based on the previous tuples received (`states`, `actions`, `rewards`, `next_states`, and `dones`).\n",
    "1. Update the current state to `next_states` and compound overall score with the received rewards.\n",
    "1. Move to the next episode if the environment's task is completed.\n",
    "\n",
    "Using a `deque`, keep track of the past 100 episodes and keep a general list of scores to be able to generate progression plots as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent\n",
    "\n",
    "| Hyperparamter  | Replay buffer | Mini-batch | $\\gamma$ | $\\tau$ | `Actor lr` | `Critic lr` | `weight_decay` |\n",
    "|----------------|---------------|------------|----------|--------|------------|-------------|----------------|\n",
    "| **Size/Value** |           1MM |         64 |     0.99 | 0.0001 |     0.0001 |      0.0003 |         0.0001 |\n",
    "\n",
    "#### Methods (of each Agent)\n",
    "```python\n",
    "class Agent():\n",
    "    def __init__: \n",
    "        # initializes both Actor and Critic local and target networks\n",
    "        \n",
    "    def step:     \n",
    "        # always saves experience to the ReplayBuffer, and on everytime the ReplayBuffer exceeds the batch_size\n",
    "\n",
    "    def act:      \n",
    "        # returns a real-valued (in [-1, 1]) action based on the current state and epsilon factor\n",
    "        # output is the calculated action based on the inputted state space (current time_step) through the MLP\n",
    "        # actions tend to have noise added, based on Ornstein-Uhlenbeck process\n",
    "        # lastly, the value is clipped [-1, 1] (as the noise could push the actions outside of said range)\n",
    "        \n",
    "    def learn:\n",
    "        # updates the policy and value params based on the incoming experiences\n",
    "        # Q_targs = r + \\gamma * critic_targ(s', actor_targ(s')), \n",
    "        #     where s' = next_state, actor_targ(s) yields action, critic_targ(s, a) yields Q-value\n",
    "        \n",
    "    def soft_update:\n",
    "        # performs a soft-update, which how the local and target models exchange parameters; the value of \\tau affects\n",
    "        #     how much information is shared, where the target network takes on ...\n",
    "        #     `tau * local_params + (1 - tau) * target_params`\n",
    "```\n",
    "There's also an additional `ReplayBuffer`, which stores past experiences so that learning can take place not only on immediately passing results but also those from far earlier timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Results\n",
    "\n",
    "DDPG ran for 300 episodes, producing a final average score of `61.99`. env __solved__ @ i_episode=206, w/ avg_score=30.17 Below is a graph of the results for single agent. This is for single agent - version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"p2_contineous_plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/ppujari/drlnd-p2/blob/master/p2_contineous_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Future Work\n",
    "\n",
    "To improve scores as well as variability in this task the `Actor` and `Critic` Networks could have both been further complexified (e.g. adding more layers or more units per layer). This setup also didn't really seem to be slowing down, so increasing number of episode likely to result in higher scores.\n",
    "\n",
    "Other possibilities include the improvements made to DDPG, such as D3PG and D4PG, A3C and PPO reading through those papers leads me to believe they would have produced better results, I am trying to implement PPO based on the openAI blog now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
